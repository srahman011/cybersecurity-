{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Product Review Classification for E-commerce\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7VIyvY-JOdFj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "424bb5f3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"m-ric/amazon_product_reviews_datafiniti\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abb5aa5b"
      },
      "source": [
        "print(\"Dataset structure:\\n\", ds)\n",
        "print(\"\\nFeatures of the training split:\\n\", ds['train'].features)\n",
        "print(\"\\nFirst 5 rows of the training split:\\n\", ds['train'][:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values"
      ],
      "metadata": {
        "id": "rYesmVAGCj6_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2f82374"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the 'train' split to a pandas DataFrame\n",
        "df_train = ds['train'].to_pandas()\n",
        "\n",
        "# Calculate the number of missing values for each column\n",
        "missing_values_count = df_train.isnull().sum()\n",
        "\n",
        "# Calculate the percentage of missing values for each column\n",
        "total_rows = len(df_train)\n",
        "missing_values_percentage = (missing_values_count / total_rows) * 100\n",
        "\n",
        "# Create a DataFrame to display the missing values information\n",
        "missing_info = pd.DataFrame({\n",
        "    'Missing Count': missing_values_count,\n",
        "    'Missing Percentage': missing_values_percentage\n",
        "})\n",
        "\n",
        "# Filter to show only columns with missing values (optional, but good for clarity)\n",
        "missing_info = missing_info[missing_info['Missing Count'] > 0]\n",
        "\n",
        "print(\"Missing Values Information (Count and Percentage):\\n\", missing_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3cfd642"
      },
      "source": [
        "df_train['reviews.numHelpful'] = df_train['reviews.numHelpful'].fillna(0)\n",
        "\n",
        "print(\"Missing values in 'reviews.numHelpful' after imputation:\", df_train['reviews.numHelpful'].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1163e29"
      },
      "source": [
        "print(\"DataFrame Info:\")\n",
        "df_train.info()\n",
        "\n",
        "print(\"\\nDataFrame dtypes:\")\n",
        "print(df_train.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5638349a"
      },
      "source": [
        "print(\"Descriptive statistics for 'reviews.numHelpful':\\n\", df_train['reviews.numHelpful'].describe())\n",
        "\n",
        "print(\"\\nUnique values and counts for 'brand':\\n\", df_train['brand'].value_counts())\n",
        "\n",
        "print(\"\\nUnique values and counts for 'primaryCategories':\\n\", df_train['primaryCategories'].value_counts())\n",
        "\n",
        "print(\"\\nUnique values and counts for 'reviews.rating':\\n\", df_train['reviews.rating'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f1ee039"
      },
      "source": [
        "category_counts = df_train['primaryCategories'].value_counts()\n",
        "unique_categories = list(category_counts.keys())\n",
        "\n",
        "# Create a new column for each unique category, indicating its presence\n",
        "for category in unique_categories:\n",
        "    df_train[category] = df_train['primaryCategories'].apply(lambda x: 1 if category in x else 0)\n",
        "\n",
        "print(\"First 5 rows with new category columns:\")\n",
        "print(df_train[['primaryCategories'] + unique_categories].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1438b1b5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='reviews.rating', data=df_train, palette='viridis', hue='reviews.rating', legend=False)\n",
        "plt.title('Distribution of Review Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52e61573"
      },
      "source": [
        "category_counts_df = df_train[unique_categories].sum().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.barplot(x=category_counts_df.index, y=category_counts_df.values, palette='coolwarm', hue=category_counts_df.index, legend=False)\n",
        "plt.title('Distribution of Product Categories')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of Products')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ad3b698"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_analyzer = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "print(\"Pre-trained multilingual sentiment analysis model loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0e8b3c5"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"m-ric/amazon_product_reviews_datafiniti\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36c986e0"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()  # Convert to string and lowercase\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with single space and strip whitespace\n",
        "    return text\n",
        "\n",
        "def map_rating_to_sentiment(rating):\n",
        "    if rating in [1, 2]:\n",
        "        return 'negative'\n",
        "    elif rating == 3:\n",
        "        return 'neutral'\n",
        "    elif rating in [4, 5]:\n",
        "        return 'positive'\n",
        "    return None\n",
        "\n",
        "# Re-apply text cleaning and sentiment mapping to the ds object to ensure 'cleaned_text' and 'sentiment' exist\n",
        "for split in ds.keys():\n",
        "    ds[split] = ds[split].map(lambda x: {'cleaned_text': clean_text(x['reviews.text'])})\n",
        "    ds[split] = ds[split].map(lambda x: {'sentiment': map_rating_to_sentiment(x['reviews.rating'])})\n",
        "\n",
        "# Recreate df_train from the updated ds['train'] to include 'cleaned_text' and 'sentiment'\n",
        "df_train = ds['train'].to_pandas()\n",
        "\n",
        "batch_size = 32 # Adjust batch size based on available memory\n",
        "predicted_sentiments = []\n",
        "\n",
        "# Process texts in batches\n",
        "for i in range(0, len(df_train), batch_size):\n",
        "    batch_texts = df_train['cleaned_text'][i:i+batch_size].tolist()\n",
        "    if not batch_texts: # Skip empty batches\n",
        "        continue\n",
        "    # Ensure texts are strings, handling potential None or non-string values\n",
        "    batch_texts = [str(text) if text is not None else '' for text in batch_texts]\n",
        "\n",
        "    predictions = sentiment_analyzer(batch_texts, truncation=True)\n",
        "    predicted_sentiments.extend([pred['label'] for pred in predictions])\n",
        "\n",
        "# Map the model's star ratings to our sentiment labels\n",
        "def map_model_output_to_sentiment(label):\n",
        "    if label in ['1 star', '2 stars']:\n",
        "        return 'negative'\n",
        "    elif label == '3 stars':\n",
        "        return 'neutral'\n",
        "    elif label in ['4 stars', '5 stars']:\n",
        "        return 'positive'\n",
        "    return None\n",
        "\n",
        "df_train['predicted_sentiment'] = [map_model_output_to_sentiment(label) for label in predicted_sentiments]\n",
        "\n",
        "print(\"Sentiment predictions generated and mapped to 'predicted_sentiment' column.\")\n",
        "print(\"First 5 predicted sentiments:\")\n",
        "print(df_train['predicted_sentiment'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a43d98c"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Ensure both columns are not null for evaluation\n",
        "df_eval_actual = df_train[df_train['sentiment'].notna() & df_train['predicted_sentiment'].notna()]\n",
        "\n",
        "# Calculate Accuracy Score\n",
        "accuracy_actual = accuracy_score(df_eval_actual['sentiment'], df_eval_actual['predicted_sentiment'])\n",
        "\n",
        "# Calculate F1-score (weighted to account for class imbalance)\n",
        "f1_actual = f1_score(df_eval_actual['sentiment'], df_eval_actual['predicted_sentiment'], average='weighted')\n",
        "\n",
        "print(f\"Actual Model Accuracy: {accuracy_actual:.4f}\")\n",
        "print(f\"Actual Model F1-score (weighted): {f1_actual:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "677d72d3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the unique sentiment labels for consistent ordering\n",
        "labels = ['positive', 'neutral', 'negative']\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm_actual = confusion_matrix(df_eval_actual['sentiment'], df_eval_actual['predicted_sentiment'], labels=labels)\n",
        "\n",
        "print(\"Actual Confusion Matrix:\")\n",
        "print(cm_actual)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_actual, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Actual Confusion Matrix')\n",
        "plt.xlabel('Predicted Sentiment')\n",
        "plt.ylabel('True Sentiment')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfa179ea"
      },
      "source": [
        "\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The pre-trained sentiment analysis model successfully generated predictions after resolving initial dependency and long sequence handling issues.\n",
        "*   The model achieved an accuracy of **0.8133** on the `df_train` dataset.\n",
        "*   The weighted F1-score for the model was **0.8255**, indicating good overall performance across sentiment classes.\n",
        "*   The confusion matrix revealed the following:\n",
        "    *   **3917** positive reviews were correctly classified as positive.\n",
        "    *   **282** neutral reviews were correctly classified as neutral.\n",
        "    *   **681** negative reviews were correctly classified as negative.\n",
        "*   Significant misclassifications occurred, particularly with positive reviews being misclassified as neutral (475 instances) or negative (173 instances).\n",
        "*   Neutral reviews showed substantial misclassification, with 170 instances predicted as positive and 175 instances predicted as negative, suggesting the model struggles to distinguish neutral sentiment from positive or negative.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Further investigation into the misclassified neutral reviews could reveal patterns or specific linguistic cues that the model struggles to interpret, potentially informing fine-tuning strategies or feature engineering.\n",
        "*   Analyzing the examples where positive sentiment was misclassified as neutral or negative could help refine the sentiment mapping rules or improve the model's ability to discern nuanced positive expressions.\n"
      ]
    }
  ]
}